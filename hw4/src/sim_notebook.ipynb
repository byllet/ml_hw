{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1rasi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from SimCLR.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f167d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got\n"
     ]
    }
   ],
   "source": [
    "path = \"D:\\ML\\ml_hw\\hw4\\images_background\"\n",
    "train_dataset, valid_dataset, test_dataset = get_datasets(path)\n",
    "print(\"got\")\n",
    "\n",
    "\n",
    "with open(\"D:\\ML\\ml_hw\\hw4\\src\\SimCLR\\hyp_params.yaml\", 'r') as f:\n",
    "    hyp = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                                batch_size=hyp['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                num_workers=hyp['n_workers'],\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True\n",
    "                            )\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                                batch_size=hyp['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                num_workers=hyp['n_workers'],\n",
    "                                pin_memory=True,\n",
    "                                drop_last=True\n",
    "                            )\n",
    "\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                                 batch_size=hyp['batch_size'],\n",
    "#                                 shuffle=True,\n",
    "#                                 num_workers=hyp['n_workers'],\n",
    "#                                 pin_memory=True,\n",
    "#                                 drop_last=True\n",
    "#                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BaseTrainProcess(hyp, train_loader, valid_loader)\n",
    "train_losses, valid_losses = trainer.run()\n",
    "\n",
    "classifier = ClassifierCLR(trainer.model.encoder, trainer.model.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b308b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierCLRTrainer:\n",
    "    def __init__(self, model, hyp, train_loader, valid_loader):\n",
    "        self.best_loss = 1e100\n",
    "        self.best_acc = 0.0\n",
    "        self.current_epoch = -1\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        self.hyp = hyp\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self._init_model()\n",
    "\n",
    "    \n",
    "    def _init_model(self):\n",
    "        model_params = [params for params in self.model.parameters() if params.requires_grad]\n",
    "        self.optimizer = torch.optim.AdamW(model_params, lr=self.hyp['lr'], weight_decay=self.hyp['weight_decay'])\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        self.warmupscheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda epoch: (epoch + 1) / 10.0)\n",
    "        self.mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            500,\n",
    "            eta_min=0.05,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "        accuracy = 0\n",
    "        proc_accuracy = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader),\n",
    "                    desc=f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, label, img) in pbar:\n",
    "            xi, label = xi.to(self.device), label.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                out = self.model(xi)\n",
    "                loss = self.criterion(out, label)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            _, pred = torch.softmax(out.detach(), dim=1).topk(k=1)\n",
    "            cur_accuracy = accuracy_score(label.detach().cpu(), pred.detach().cpu())\n",
    "            accuracy += cur_accuracy\n",
    "            proc_accuracy = (proc_accuracy * idx + cur_accuracy) / (idx + 1)\n",
    "\n",
    "            s = f'Train {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}, Acc: {proc_accuracy:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.train_loader)\n",
    "        accuracy = accuracy / len(self.train_loader)\n",
    "\n",
    "        return [cum_loss, accuracy]\n",
    "\n",
    "    def valid_step(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        cum_loss = 0.0\n",
    "        proc_loss = 0.0\n",
    "\n",
    "        accuracy = 0\n",
    "        proc_accuracy = 0.0\n",
    "\n",
    "        pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader),\n",
    "                    desc=f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}')\n",
    "        for idx, (xi, xj, label, img) in pbar:\n",
    "            xi, label = xi.to(self.device), label.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                out = self.model(xi)\n",
    "                loss = self.criterion(out, label)\n",
    "\n",
    "            cur_loss = loss.detach().cpu().numpy()\n",
    "            cum_loss += cur_loss\n",
    "            proc_loss = (proc_loss * idx + cur_loss) / (idx + 1)\n",
    "\n",
    "            _, pred = torch.softmax(out.detach(), dim=1).topk(k=1)\n",
    "            cur_accuracy = accuracy_score(label.detach().cpu(), pred.detach().cpu())\n",
    "            accuracy += cur_accuracy\n",
    "            proc_accuracy = (proc_accuracy * idx + cur_accuracy) / (idx + 1)\n",
    "\n",
    "            s = f'Valid {self.current_epoch}/{self.hyp[\"epochs\"] - 1}, Loss: {proc_loss:4.3f}, Acc: {proc_accuracy:4.3f}'\n",
    "            pbar.set_description(s)\n",
    "\n",
    "        cum_loss /= len(self.valid_loader)\n",
    "        accuracy /= len(self.valid_loader)\n",
    "        return [cum_loss, accuracy]\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        for epoch in range(self.hyp['epochs']):\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            loss_train = self.train_step()\n",
    "            train_losses.append(loss_train)\n",
    "\n",
    "            if epoch < 10:\n",
    "                self.warmupscheduler.step()\n",
    "            else:\n",
    "                self.mainscheduler.step()\n",
    "\n",
    "            lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            loss_valid = self.valid_step()\n",
    "            valid_losses.append(loss_valid)\n",
    "\n",
    "            # self.save_checkpoint(loss_valid, best_w_path)\n",
    "\n",
    "        # self.save_model(last_w_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490dcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0/0, Loss: 6.818, Acc: 0.004: 100%|██████████| 120/120 [00:36<00:00,  3.30it/s]\n",
      "Valid 0/0, Loss: 7.545, Acc: 0.002: 100%|██████████| 30/30 [00:09<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[6.817670567830404, 0.0036458333333333334]], [[7.545482317606608, 0.0015625]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_trainer = ClassifierCLRTrainer(classifier, hyp, train_loader, valid_loader)\n",
    "print(classifier_trainer.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ef017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
